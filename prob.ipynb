{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "import random\n",
    "\n",
    "import torch\n",
    "import torch.nn.functional as F\n",
    "\n",
    "from time import perf_counter\n",
    "\n",
    "from transformers import AutoModelForCausalLM\n",
    "from transformers import AutoTokenizer\n",
    "\n",
    "from IPython.display import clear_output\n",
    "\n",
    "from LogitWrappers import *\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tokenizer = AutoTokenizer.from_pretrained(\"concedo/OPT-19M-ChatSalad\")\n",
    "model = AutoModelForCausalLM.from_pretrained(\"concedo/OPT-19M-ChatSalad\", return_dict_in_generate=True)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "def summary(xs, titles):\n",
    "    for x, title in zip(xs, titles):\n",
    "        print(' ' * (15 - (len(title) + 1)), f'{title}:', end=' ')\n",
    "        # for fn in (torch.max, torch.mean, torch.median, len):\n",
    "            # print(f'{fn.__name__}\\t{fn(x.float()):.2f}')\n",
    "        print(x.item())\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "def best_k_p(logits, golden):\n",
    "    sorted_logits, sorted_indices = torch.sort(logits, descending=True)\n",
    "    cumulative_probs = torch.cumsum(F.softmax(sorted_logits, dim=-1), dim=-1)\n",
    "\n",
    "    probs = sorted_logits.softmax(dim=-1)\n",
    "\n",
    "    probs_max = probs[..., 0, None]\n",
    "    prob_golden = probs[sorted_indices == golden]\n",
    "\n",
    "    print(' ' * (15 - len('prob max:')), 'prob max:', probs_max.item())\n",
    "    print(' ' * (15 - len('prob golden:')), 'prob golden:', prob_golden.item())\n",
    "    print(' ' * (15 - len('cumulative_probs:')), 'cumulative probs:', cumulative_probs.shape)\n",
    "    \n",
    "    print()\n",
    "\n",
    "    temp = (prob_golden *  probs_max) / 2\n",
    "    tk = (sorted_indices == golden).nonzero()[:, 1]\n",
    "    tp = cumulative_probs[sorted_indices == golden]\n",
    "\n",
    "    return temp, tk, tp\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.10483854499761946 sec to get logits\n",
      "\n",
      "       prob max: 0.07534100860357285\n",
      "    prob golden: 4.79663722217083e-05\n",
      " cumulative probs: torch.Size([1, 50266])\n",
      "\n",
      "           temp: 1.806917452995549e-06\n",
      "              k: 1604\n",
      "              p: 0.8144427537918091\n"
     ]
    }
   ],
   "source": [
    "prompt = \"Echo Sohma sat at her desk,\"\n",
    "\n",
    "input_ids = tokenizer(prompt, return_tensors=\"pt\").input_ids\n",
    "\n",
    "start_time = perf_counter()\n",
    "outputs = model(input_ids)\n",
    "print(perf_counter() - start_time, 'sec to get logits', end='\\n\\n')\n",
    "\n",
    "next_token_logits = outputs[0][:, -1, :]\n",
    "\n",
    "summary(best_k_p(next_token_logits, torch.tensor([19311]).unsqueeze(1)), ['temp', 'k', 'p'])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "metadata": {},
   "outputs": [],
   "source": [
    "class AdvancedRepetitionPenaltyLogitsProcessor():\n",
    "    def __init__(self, penalty: int, penalty_range: int, penalty_slope: int):\n",
    "        self.penalty = penalty\n",
    "        self.penalty_range = int(penalty_range)\n",
    "        self.penalty_slope = penalty_slope\n",
    "\n",
    "    def __call__(self, input_ids: torch.LongTensor, scores: torch.FloatTensor) -> torch.FloatTensor:\n",
    "        clipped_penalty_range = min(input_ids.shape[-1], self.penalty_range)\n",
    "\n",
    "        if self.penalty != 1.0:\n",
    "            if self.penalty_range > 0:\n",
    "                if clipped_penalty_range < input_ids.shape[1]:\n",
    "                    input_ids = input_ids[..., -clipped_penalty_range:]\n",
    "\n",
    "                if self.penalty_slope != 0:\n",
    "                    _penalty = (torch.arange(self.penalty_range, dtype=scores.dtype, device=scores.device)/(self.penalty_range - 1)) * 2. - 1\n",
    "                    _penalty = (self.penalty_slope * _penalty) / (1 + torch.abs(_penalty) * (self.penalty_slope - 1))\n",
    "                    _penalty = 1 + ((_penalty + 1) / 2).unsqueeze(0) * (self.penalty - 1)\n",
    "                    self.penalty = _penalty[..., -clipped_penalty_range]\n",
    "\n",
    "            score = torch.gather(scores, 1, input_ids)\n",
    "            score = torch.where(score <= 0, score * self.penalty, score / self.penalty)\n",
    "            scores.scatter_(1, input_ids, score)\n",
    "\n",
    "        return scores\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 71,
   "metadata": {},
   "outputs": [],
   "source": [
    "sample_order = [\n",
    "    ['Temperature', TemperatureLogitsWarper(temperature=0.9)],\n",
    "    ['Repetition Penalty', AdvancedRepetitionPenaltyLogitsProcessor(penalty_range=1024, penalty=1.1, penalty_slope=0.7)],\n",
    "    ['Top P', TopPLogitsWarper(top_p=0.9)],\n",
    "    ['Top K', TopKLogitsWarper(top_k=15)],\n",
    "    ['Top A', TopALogitsWarper(top_a=0.7)],\n",
    "    ['Typical Sampling', TypicalLogitsWarper(typical=1.0)],\n",
    "    ['Temperature', TemperatureLogitsWarper(temperature=5.0)],\n",
    "    ['Top P', TopPLogitsWarper(top_p=0.6)],\n",
    "    ['Tail Free', TailFreeLogitsWarper(tfs=1.0)],\n",
    "    ['Min Length', MinLengthLogitsProcessor(min_length=10, eos_token_id=tokenizer.eos_token_id)]\n",
    "]\n",
    "\n",
    "prompt = \"this model\\n\"\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(prompt, end='')\n",
    "\n",
    "for _ in range(100):\n",
    "    input_ids = tokenizer(prompt, return_tensors=\"pt\").input_ids\n",
    "\n",
    "    with torch.no_grad():\n",
    "        start_time = perf_counter()\n",
    "        outputs = model(input_ids)\n",
    "        print(perf_counter() - start_time, 'sec to get logits', end='\\n\\n')\n",
    "\n",
    "        next_token_logits = outputs[0][:, -1, :]\n",
    "\n",
    "        for [sampler_name, sampler] in [['None', lambda input_ids, scores: scores], *sample_order]:\n",
    "            next_token_logits = sampler(input_ids=input_ids, scores=next_token_logits)\n",
    "\n",
    "            probs_for_sampler = F.softmax(next_token_logits, dim=-1).squeeze(0)\n",
    "\n",
    "            prob_list_for_sampler = []\n",
    "\n",
    "            for (prb, idx) in zip(probs_for_sampler, range(probs_for_sampler.shape[0])):\n",
    "                if (prb.item() == 0):\n",
    "                    continue\n",
    "\n",
    "                prob_list_for_sampler.append(((idx, tokenizer.decode(idx)), prb.item()))\n",
    "\n",
    "            print(f\"{sampler_name}:{' ' * (20 - len(sampler_name))}\", end='')\n",
    "\n",
    "            tokens = []\n",
    "\n",
    "            for ((id, token), prob) in sorted(prob_list_for_sampler, key=lambda x: x[1], reverse=True)[:7]:\n",
    "                tokens.append(f\"{json.dumps(token)} ({'{:.4f}'.format(round(prob, 4))})\")\n",
    "\n",
    "            print(', '.join(tokens), f'...{len(prob_list_for_sampler) - 7} more' if len(prob_list_for_sampler) > 10 else '')\n",
    "\n",
    "        print()\n",
    "\n",
    "        print(f\"Final probabilities:{' ' * (20 - len('Final probablities:'))}\", end='')\n",
    "\n",
    "        for ((id, token), prob) in sorted(prob_list_for_sampler, key=lambda x: x[1], reverse=True):\n",
    "            print(f\"{json.dumps(token)} ({'{:.4f}'.format(round(prob, 4))})\", end=', ')\n",
    "\n",
    "        print()\n",
    "        print()\n",
    "\n",
    "        sampled_token_org = tokenizer.decode(torch.multinomial(F.softmax(outputs[0][:, -1, :], dim=-1).squeeze(0), 1)[0])\n",
    "        sampled_token = tokenizer.decode(torch.multinomial(F.softmax(next_token_logits, dim=-1), 1)[0])\n",
    "\n",
    "        rand = random.random()\n",
    "\n",
    "        prompt += sampled_token\n",
    "\n",
    "        print('Sampled (Before samplers):', json.dumps(sampled_token_org))\n",
    "        print('Sampled (After samplers) :', json.dumps(sampled_token))\n",
    "\n",
    "        print('Taken:', sampled_token if rand > 0.5 else sampled_token_org)\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.10.2 ('venv': venv)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.2"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "2cc03e3db703c31f57d969ff2129c8806a0f20938828ade0a91a9159c61623e0"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
